{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851568f6-a54c-4bc6-a6c0-23c684c5817f",
   "metadata": {},
   "source": [
    "Other resources:\n",
    " - [Other people using Yelp data](https://blog.michaelckennedy.net/2017/06/21/yelp-reviews-authorship-attribution-with-python-and-scikit-learn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "af88c099-5e45-4e52-aef6-64610f13b488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc7d62-1752-42bc-b8f0-ceef8865671e",
   "metadata": {},
   "source": [
    "# 0. Load and Clean Data\n",
    "\n",
    "We will concatenate the CSVs (instead of joining raw data) to cut down on RAM compute needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d80bd7d7-fa34-4cef-8fa3-af28afe85e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('yelp/2006_2005_final_dd.csv')\n",
    "df2 = pd.read_csv('yelp/2007_2006_final_dd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cf565a24-22e0-4b3b-98fd-58dc34a8f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['year'] = 2006\n",
    "df2['year'] = 2007\n",
    "\n",
    "df = pd.concat([df1,df2])\n",
    "\n",
    "df = df.sort_values('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "519f734f-59f1-4fc5-b70e-d102b71a92e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(854, 22)\n",
      "(3853, 21)\n"
     ]
    }
   ],
   "source": [
    "print(df1.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3e25b196-4e03-474b-bb07-7ea94208d8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4707, 22)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ac0cb871-f223-46f4-ba1b-1db36492cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = df1.sort_values('date')\n",
    "# df2 = df2.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ff7f22-be14-4797-9f0f-b43a69575499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['slice'] -> ['2006', '2007']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0d7ef54a-9a35-42cb-8ca0-9d524db00a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca1689e6-6238-47b1-a423-c249220f995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['categories_lst'] = df1['categories'].apply(lambda x: x.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18d8f53f-e984-4283-91cf-571aad8be91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we encode groups?\n",
    "\n",
    "ethnic_categories = [\n",
    "'American (Traditional)',\n",
    "    'American (New)',\n",
    "    'Italian',\n",
    "    'Mexican',\n",
    "    'Japanese',\n",
    "    'Chinese',\n",
    "    'Southern',\n",
    "    'Vietnamese',\n",
    "    'Asian Fusion',\n",
    "    'Mediterranean',\n",
    "    'Thai'\n",
    "]\n",
    "\n",
    "# TODO: figure out how this works in the eval step\n",
    "def is_subgroup(s,category):\n",
    "    return category in row[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5fe12-648a-4c73-97c7-c35a0d05351f",
   "metadata": {},
   "source": [
    "Copied from [this Jupyter notebook](https://github.com/ahegel/yelp-dataset/blob/master/Predicting%20Star%20Ratings.ipynb) but couldn't get it working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79744113-3218-4246-bd3b-2a8d5aebbb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add categories and attributes\n",
    "\n",
    "# df1['categories_clean'] = map(lambda x: '|'.join(x), df1['categories'])\n",
    "# df1 = df1.categories_clean.str.get_dummies(sep='|')\n",
    "# # merge\n",
    "# # business_df = business_df.merge(categories_df, left_index=True, right_index=True)\n",
    "# # remove intermediate columns (no longer needed)\n",
    "# df1.drop(['categories', 'categories_clean'], axis=1, inplace=True)\n",
    "# df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df653120-f837-4e28-b1cf-b185f6da4cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# business_df = business_df.join(pd.DataFrame(business_df['attributes'].to_dict()).T)\n",
    "# # further split sub-attributes into their own columns\n",
    "# cols_to_split = ['BusinessParking', 'Ambience', 'BestNights', 'GoodForMeal', 'HairSpecializesIn', 'Music']\n",
    "# for col_to_split in cols_to_split:\n",
    "#     new_df = pd.DataFrame(business_df[col_to_split].to_dict()).T\n",
    "#     new_df.columns = [col_to_split + '_' + str(col) for col in new_df.columns]\n",
    "#     business_df = business_df.join(new_df)\n",
    "\n",
    "# business_df.drop(['attributes'] + cols_to_split, axis=1, inplace=True)\n",
    "# business_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1aeac456-6c5f-4f9e-9c91-9915d61ed1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tab_cols = ['stars_x','useful', 'funny', 'cool']\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "X_text_vec = vec.fit_transform(df['text']).tocsr()\n",
    "X_tab = df[tab_cols].values\n",
    "X = sparse.hstack((X_text_vec, X_tab)).tocsr()\n",
    "\n",
    "y = (df['stars_y'].values >= 3.).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a7511-9ca8-4e93-bd34-788ae444c32c",
   "metadata": {},
   "source": [
    "# 1. Modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b5b58d-a23c-4969-a372-f546babbb702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for S1: subsample N from 0 to 1000; train on that and report test perf\n",
    "# \n",
    "# add S2 and learn for S1 + S2[:100], S1 + S2[:200]\n",
    "# continue for all slices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d07fad0-0bcf-4d69-84d8-42ab6f6959b2",
   "metadata": {},
   "source": [
    "## 1.1 Using actual Yelp dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "59a68246-a34a-4594-bd07-6e18dfa8962f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4707, 22)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "17af8808-b291-4bf8-b041-06ae4b516dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(854, 22) (3853, 21)\n",
      "4707\n"
     ]
    }
   ],
   "source": [
    "print(df1.shape, df2.shape)\n",
    "print(854+3853)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "63482694-6f37-460d-b623-43eaf2f8a158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        , 284.66666667, 569.33333333, 854.        ])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0, 854,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "00d1bfc0-84f0-4607-b743-953bbfcda0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 854.        , 2138.33333333, 3422.66666667, 4707.        ])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(854,4707,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a46939a6-16a0-4747-a24f-39b852962716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,  250,  500,  750, 1000, 1250, 1500, 1750, 2000, 2250, 2500,\n",
       "       2750, 3000, 3250, 3500, 3750, 4000, 4250, 4500])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0,4707,250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5283739e-3efd-46cd-9ee8-4caa0a2ee1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_counts_dict = df['year'].value_counts().to_dict()\n",
    "year_counts_dict\n",
    "\n",
    "year_idx_dict = {year: np.where(df['year'].values == year)[0] for year in df['year'].unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "59c40e90-f667-4711-a8c6-ea6cf38db034",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 900\n",
    "ignore_idx = [0,4,2,100]\n",
    "year_idx_dict_ignore = {k:[i for i in v if i not in ignore_idx] for k,v in year_idx_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9d9df34a-0a71-402c-824b-d2cb2b6f33b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# within avail idx, within year, then sample\n",
    "# train_set_idx = []\n",
    "\n",
    "def get_avail_idx(num, year_idx_dict, ignore_idx, avail_years):\n",
    "    \"\"\"\n",
    "    input:\n",
    "     - num: int of samples needed\n",
    "     - year_idx_dict: dict of {year: list of idx} for the df\n",
    "     - ignore_idx: list of idx to ignore\n",
    "     - avail_years: list of years that are considered\n",
    "    \n",
    "    output:\n",
    "     - avail_dict: dict of {num to sample: list of valid idx}\n",
    "    \"\"\"\n",
    "    year_idx_dict_ignore = {k:[i for i in v if i not in ignore_idx] for k,v in year_idx_dict.items()}\n",
    "    avail_dict = {}\n",
    "    remaining = num\n",
    "    for year in sorted(year_idx_dict_ignore.keys()):\n",
    "        if remaining > 0:\n",
    "            avail_year_idx = year_idx_dict_ignore[year]\n",
    "            avail_year_num_idx = len(avail_year_idx)\n",
    "            \n",
    "            use_this_year = min(remaining, avail_year_num_idx)\n",
    "            avail_dict[use_this_year] = avail_year_idx\n",
    "            remaining -= avail_year_num_idx\n",
    "    return avail_dict\n",
    "\n",
    "def get_sample_avail_idx(avail_dict):\n",
    "    avail_idx_lst = list()\n",
    "    \n",
    "    for num_to_sample, lst_to_sample in avail_dict.items():\n",
    "        sample_idx = np.random.choice(lst_to_sample, size=num_to_sample, replace=False)\n",
    "        avail_idx_lst.append(sample_idx)\n",
    "        \n",
    "    avail_idx = np.concatenate(avail_idx_lst)\n",
    "    return avail_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0498ba63-9137-4779-aac9-b321acfeebc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([851, 149])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = get_avail_idx(1000, year_idx_dict, [0,1,100],avail_years=[2006,2007])\n",
    "test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "05b19189-1db7-45e7-a199-0787ff8063bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.coo.coo_matrix"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8a5223a9-5d65-48c0-929b-00493a73006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# held out set of 20% test\n",
    "# train_size = 100, 200, 300, 400, 500, 600 (4 chunks)\n",
    "\n",
    "# reference test_set = 100 random points from latest slice\n",
    "# generalized test_set = 100 random points from total dataset\n",
    "# source test_set = 100 random points from each training run\n",
    "\n",
    "test_set_size = 100\n",
    "N = df.shape[0]\n",
    "\n",
    "# ref: take 100 from df\n",
    "max_year = max(df['year'].values)\n",
    "ref_test_idx = np.random.choice(year_idx_dict[max_year], size=test_set_size, replace=False)\n",
    "\n",
    "# gen: take 100 from df1 and df2\n",
    "gen_test_idx = np.random.choice(range(N), size=test_set_size, replace=False)\n",
    "\n",
    "\n",
    "step_size = 250\n",
    "\n",
    "for train_N in [2000]:\n",
    "# for train_N in np.arange(step_size, N,step_size):\n",
    "    ignore_idx = np.concatenate((ref_test_idx,gen_test_idx))\n",
    "    total_idx_sample_dict = get_avail_idx(train_N+test_set_size, year_idx_dict, ignore_idx, avail_years=[2006, 2007])\n",
    "    total_idx = get_sample_avail_idx(total_idx_sample_dict)\n",
    "    \n",
    "    train_idx = total_idx[test_set_size:]\n",
    "    \n",
    "    # source: take 100 from each train run\n",
    "    source_test_idx = total_idx[:test_set_size]\n",
    "    \n",
    "    X_train = X[train_idx].toarray()\n",
    "    y_train = y[train_idx]\n",
    "    \n",
    "    X_source, y_source = X[source_test_idx].toarray(), y[source_test_idx]\n",
    "    X_ref, y_ref = X[ref_test_idx].toarray(), y[ref_test_idx]\n",
    "    X_gen, y_gen = X[gen_test_idx].toarray(), y[gen_test_idx]\n",
    "    \n",
    "    model = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    yhat_source = model.predict(X_source)\n",
    "    yhat_ref = model.predict(X_ref)\n",
    "    yhat_gen = model.predict(X_gen)\n",
    "    \n",
    "    acc_source = accuracy_score(yhat_source, y_source)\n",
    "    acc_ref = accuracy_score(yhat_ref, y_ref)\n",
    "    acc_gen = accuracy_score(yhat_gen, y_gen)\n",
    "    \n",
    "    results = {\n",
    "        'acc_source': acc_source,\n",
    "        'acc_ref': acc_ref,\n",
    "        'acc_gen': acc_gen,\n",
    "        'source_test_idx': source_test_idx,\n",
    "        'gen_test_idx': gen_test_idx,\n",
    "        'ref_test_idx': ref_test_idx,\n",
    "        'yhat_source': yhat_source,\n",
    "        'yhat_ref': yhat_ref,\n",
    "        'yhat_gen': yhat_gen,\n",
    "        'y_source': y_source,\n",
    "        'y_ref': y_ref,\n",
    "        'y_gen': y_gen\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "37e3488f-9cca-4193-bd28-9c1772dd02d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93 0.93 0.9\n"
     ]
    }
   ],
   "source": [
    "print(acc_source, acc_ref, acc_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f681998-30f8-4eb1-b718-7138cb91533b",
   "metadata": {},
   "source": [
    "## 1.2 Using subsampled random slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f135c81-2bce-4798-a36d-2d4359944040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
